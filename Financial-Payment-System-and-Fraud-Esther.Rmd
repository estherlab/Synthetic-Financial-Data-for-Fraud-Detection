---
title: "Financial Payment System and Fraud"
author: "Esther Roseline"
date: "1/9/2020"
output: pdf_document
---


## Introduction

This is a report for Capstone Project II of Harvardx Data Science Program. The purpose is to develop a machine learning model to detect fraud in a financial payment system. We are using the "Synthetic data from a financial payment system" dataset downloaded from kaggle.com. Here is the Github link (https://github.com/estherlab/Synthetic-Financial-Data-for-Fraud-Detection.git). The key steps we will perform are, as follows:

* Importing the Dataset and essential libraries
* Performing Data Exploration
* Preprocessing
* Splitting Dataset into training and test sets
* Testing different models
* Measuring the error


## Methods/Analysis

### Dataset and Libraries

We are importing the dataset that contains aggregated transactional data of bank payments and the essential libraries

```{r}
library(ranger)
library(caret)
library(data.table)
library(tidyverse)
library(ggplot2)
library(dplyr)
banksim_data <- read.csv("./banksim1/bs140513_032310.csv")
```

### Data Exploration

We will now explore the data contained in the banksim_data dataframe. We will proceed by displaying the banksim_data by dimension, summary, and printing the first and last six lines of the dataframe. 

```{r echo=FALSE}
dim(banksim_data)
summary(banksim_data)
head(banksim_data)
tail(banksim_data)
```

The number of transactions that have been observed as fraud  vs normal is, as follows: (0 means normal, 1 means fraud)

```{r echo=FALSE}
table(banksim_data$fraud)
```

Hence the percentage of fraud vs. normal transactions is: 

```{r echo=FALSE}
percent_fraud <- round(7200 / (7200 + 587443) * 100, 2)
percent_normal <- 100 - percent_fraud
```
```{r}
percent_fraud
```
```{r}
percent_normal
```

Now we will look into the variables at the dataset. There are 10 variables 
```{r echo=FALSE}
names(banksim_data)
```
In this case, we notice that the first 9 variables (step, customer, age, gender, zipcodeOri, merchant, zipMerchant, category, amount) are features, while the 10th variable is our target (fraud)

We will start looking into the variables by visualizing the distribution of each of the features to see how their variance is.

```{r echo=FALSE}
a <- banksim_data %>%
  group_by(banksim_data$amount) %>%
  summarize(n = n())
b <- ggplot(a, aes(x = a$`banksim_data$amount`, y=n)) 
b + geom_bar(stat = "identity", col = "blue") + xlab("amount") + ylab("total") + ggtitle("Amount")
```
```{r echo=FALSE}
a2 <- banksim_data %>%
  group_by(banksim_data$category) %>%
  summarize(n = n())
b2 <- ggplot(a2, aes(x = a2$`banksim_data$category`, y=n)) 
b2 + geom_bar(stat = "identity", col = "black") + xlab("category") + ylab("total") + theme(axis.text.x = element_text(angle = 45, hjust = 1)) + ggtitle("Category")
```
```{r echo=FALSE}
a3 <- banksim_data %>%
  group_by(banksim_data$merchant) %>%
  summarize(n = n())
b3 <- ggplot(a3, aes(x = a3$`banksim_data$merchant`, y=n)) 
b3 + geom_bar(stat = "identity", col = "red") + xlab("Merchant") + ylab("total") + ggtitle("Merchant")
```
```{r echo=FALSE}
a4 <- banksim_data %>%
  group_by(banksim_data$zipMerchant) %>%
  summarize(n = n())
b4 <- ggplot(a4, aes(x = a4$`banksim_data$zipMerchant`, y=n)) 
b4 + geom_bar(stat = "identity", col = "yellow") + xlab("zipMerchant") + ylab("total") + ggtitle("ZipMerchant")
```
```{r echo=FALSE}
a5 <- banksim_data %>%
  group_by(banksim_data$zipcodeOri) %>%
  summarize(n = n())
b5 <- ggplot(a5, aes(x = a5$`banksim_data$zipcodeOri`, y=n)) 
b5 + geom_bar(stat = "identity", col = "green") + xlab("zipcodeOri") + ylab("total") + ggtitle("zipcodeOri")
```
```{r echo=FALSE}
a6 <- banksim_data %>%
  group_by(banksim_data$gender) %>%
  summarize(n = n())
b6 <- ggplot(a6, aes(x = a6$`banksim_data$gender`, y=n)) 
b6 + geom_bar(stat = "identity", col = "black") + xlab("gender") + ylab("total") + ggtitle("Gender")
```
```{r echo=FALSE}
a7 <- banksim_data %>%
  group_by(banksim_data$age) %>%
  summarize(n = n())
b7 <- ggplot(a7, aes(x = a7$`banksim_data$age`, y=n)) 
b7 + geom_bar(stat = "identity", col = "red") + xlab("Age") + ylab("total") + ggtitle("Age")
```
```{r echo=FALSE}
a8 <- banksim_data %>%
  group_by(banksim_data$customer) %>%
  summarize(n = n())
b8 <- ggplot(a8, aes(x = a8$`banksim_data$customer`, y=n)) 
b8 + geom_bar(stat = "identity", col = "blue") + xlab("Customer") + ylab("total") + ggtitle("Customer")
```
```{r echo=FALSE}
a9 <- banksim_data %>%
  group_by(banksim_data$step) %>%
  summarize(n = n())
b9 <- ggplot(a9, aes(x = a9$`banksim_data$step`, y=n)) 
b9 + geom_bar(stat = "identity", col = "red") + xlab("Step") + ylab("total") + ggtitle("Step")
```

From the visualizations, we can safely infer that there are features that won't affect our classification process/prediction so we can take out/ignore in our models:

* zipcodeOri and zipMerchant, because they are constant/only have 1 unique entry. 
* Customer, because practically anybody can have any random customer IDs, and it is better that we just account for the relevant customers' characteristics by the other features (such as gender and age). We will factor in the customers' characteristics in our model. 

## Preprocessing

We will standardize the amount data in our banksim_data, and simulatenously create a new dataset that has had zipcodeOri, zipMerchant, and Customer, and step data removed
```{r}
banksim_data$amount <- scale(banksim_data$amount)
Standardized_Data <- banksim_data[,-c(1)]
NewData_banksim <- Standardized_Data [,-c(1,4,6)]
```
Our new data now looks like as follows:
```{r echo=FALSE}
head(NewData_banksim)
```

## Splitting Datasets

We will now split our datasets into training and test sets. The test set will be 20% of the whole data set. The 80% will go to the train set.

```{r}
set.seed(1)
test_index <- createDataPartition(y = NewData_banksim$fraud, times = 1, p = 0.2, list = FALSE)
train_data <- NewData_banksim[-test_index,]
test_data <- NewData_banksim[test_index,]
```
```{r echo=FALSE}
summary(test_data)
summary(train_data)
```

## Testing different models

### First Model: Decision Tree

The first model we will try is a Decision Tree model. 

```{r}
require(tree)
library(rpart)
library(rpart.plot)
tree.banksim <- rpart(fraud~.,train_data, method='class')
```
However, plotting the classification tree produces a very hard-to-read figure
Nonetheless, let us see the summary of our classification tree: 
```{r echo=FALSE}
summary(tree.banksim)
```
From the summary, we discover in Variable Importance that actually only "amount", "merchant", and "category" which are the most important features in our dataset for predicting fraud. 

As for the result of prediction, this decision tree, however, has the classification error of:
```{r}
tree.pred <- predict(tree.banksim, test_data, type="class")
with(test_data, table(tree.pred, fraud))
```
```{r echo=FALSE}
(117386+1027)/(117386+1027+119+397)
```
Or we can see the table of probability as follows: 
```{r}
probability <- predict(tree.banksim, test_data, type='prob')
summary(probability)
```

### Logistic Regression Model

The classification error produced by Decision Tree looks not quite good. Most likely because we have a very unbalanced data as we have seen during data exploration (The fraud sample is only about 1% of the total dataset). 

So now we will turn to Logistic Regression Model. As we have discovered that the most important variables in our dataset are merchant, amount, and category, we will now include these features in our model - that is, we are to predict fraud, given/based on the merchant, amount, and category:
```{r}
fit_glm <- glm(fraud ~ merchant + amount + category, train_data, family = "binomial")
p_hat_glm <- predict(fit_glm, test_data, type = "response")
y_hat_glm <- ifelse(p_hat_glm > 0.5, 1, 0) %>% factor()
```
The accuracy of our model will now be measured by Confusion Matrix:
```{r}
confusionMatrix(data = y_hat_glm, reference = as.factor(test_data$fraud))$overall["Accuracy"]
```

The accuracy looks quite awesome. 

## Results

Based on the experiments of different models and testing them with errors and accuracy, it is found that the best prediction is produced using the **Logistic Regression Model**. First we eliminate the irrelevant variables, then we take into account all the other variables until we found the most important variables. Then we notice that our data is very unbalanced that Decision Trees model is not the best model. We then found that the Logistic Regression Model produces the good accuracy to detect fraud in our data. The final model can be demonstrated as it is being run on our train and test set, as follows: 

```{r}
fit_glm <- glm(fraud ~ merchant + amount + category, train_data, family = "binomial")
p_hat_glm <- predict(fit_glm, test_data, type = "response")
y_hat_glm <- ifelse(p_hat_glm > 0.5, 1, 0) %>% factor()
```

The prediction generated is: 
```{r}
head(y_hat_glm)
```

With the accuracy of: 
```{r}
confusionMatrix(data = y_hat_glm, reference = as.factor(test_data$fraud))$overall["Accuracy"]
```

## Conclusion 

The core of the whole process in making a model to detect fraud in financial payment system is that we must build an algorithm with the past data we have collected (with all variables related to the transaction concerned) in order to detect future possible occurences of fraud. We have reached the conclusion that in improving our models to minimize errors, there are at least two things that are most essential to be taken into account while producing detection system: we need to discover the most important variables, and take into account the unbalanced data we posisbly have. 

There are, however, some limitations on the models we develop. Some of the major limitations are: first, not all past fraud cases are or have been detected, hence our input data used to train and test might be flawed. Second, the most important variables detected by our algorithm might also be mistaken, since in real life not everything can be captured by data -- fraud, in reality, is a human behaviour problem. 

In the future, these kinds of models must also be actively analyzed by fraud experts, investigators, and behavioural analysts in order to better support data scientists in developing their models. 